{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook nous allons discuter 3 conceptes principaux afin de simplifier notre notebook. \n",
    "\n",
    "- Word vectors\n",
    "- Recurrent neural networks\n",
    "- Long short-term memory units (LSTMs). \n",
    "\n",
    "Après avoir une compréhension bien claire au differente théorie nous allons les introduire avec du code brute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir une idée comment on peut appliquer le Deep Learning, pense au différente format des données qu'on peut utiliser afain de satisfaire les différents algorithmes. Convolutionnal network utilise des matrice de pixel sous forme numérique, régression linéaire utilise des données quantitative, et reinforcement learning utilise des signaux. le thème commun c'est que tout ces valeur doivent être des valeurs scalaires, ou des matrices de valeurs scalaire. Quand on pense a l'NLP ce genre de données doit être aussi scalaires.\n",
    " \n",
    "![caption](Images/SentimentAnalysis.png)\n",
    "\n",
    "Ce genre de donnée peut être problèmatique. On ne peut jamais faire des produit scalaire et des retropropagation sur des chaines de caractères . Alors pour faciliter et rendre possible les différente opération sur ces chaine de caractère on a pensé à convertir chaque mot dans les phrase comme étant un vecteur. \n",
    "\n",
    "![caption](Images/SentimentAnalysis2.png)\n",
    "\n",
    "On peux penser par exemple que l'input de notre text est une matrice de 16 dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voulons que ces vecteurs soient créés de manière à représenter le mot et son contexte, sa signification et sa sémantique. Par exemple, nous préférerions que les vecteurs des mots «aimer» et «adorer» résident dans la même zone de l’espace vectoriel, car ils ont tous deux une définition similaire et sont tous deux utilisés dans des contextes similaires. La représentation vectorielle d'un mot est également appelée word embedding.\n",
    "\n",
    "![caption](Images/SentimentAnalysis8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voulons que ces vecteurs soient créés de telle manière qu'ils représentent en quelque sorte le mot et son contexte, son sens et sa sémantique. Par exemple, nous aimerions que les vecteurs pour les mots «aimer» et «adorer» résident dans la même zone dans l'espace vectoriel, car ils ont tous les deux des définitions similaires et sont tous deux utilisés dans des contextes similaires. La représentation vectorielle d'un mot est également connue sous le nom d'incorporation de mots.\n",
    "\n",
    "![caption](Images/SentimentAnalysis9.png)\n",
    "\n",
    "Du contexte des phrases, nous pouvons voir que les deux mots sont généralement utilisés dans des phrases avec des connotations positives et précèdent généralement les noms ou les expressions nominales. C'est une indication que les deux mots ont quelque chose en commun et peuvent éventuellement être synonymes. Le contexte est également très important lorsque l'on considère la structure grammaticale dans les phrases. La plupart des phrases suivront les paradigmes traditionnels selon lesquels les verbes suivent les noms, les adjectifs précèdent les noms, et ainsi de suite. Pour cette raison, le modèle est plus susceptible de positionner les noms dans la même zone générale que les autres noms. Le modèle intègre un grand ensemble de phrases (Wikipédia en anglais par exemple) et génère des vecteurs pour chaque mot unique du corpus. La sortie d'un modèle Word2Vec s'appelle une matrice d'inclusion.\n",
    "\n",
    "![caption](Images/SentimentAnalysis3.png)\n",
    "\n",
    "Cette matrice d'inclusion contiendra des vecteurs pour chaque mot distinct du corpus d'apprentissage. Traditionnellement, les matrices d'inclusion peuvent contenir plus de 3 millions de vecteurs de mots.\n",
    "\n",
    "Le modèle Word2Vec est entraîné en prenant chaque phrase dans l'ensemble de données, en glissant une fenêtre de taille fixe dessus, et en essayant de prédire le mot central de la fenêtre, étant donné les autres mots. En utilisant une fonction de perte et une procédure d'optimisation, le modèle génère des vecteurs pour chaque mot unique. Les détails de cette procédure de formation peuvent être un peu compliqués, donc nous allons passer à côté des détails pour l'instant, mais la principale conclusion ici est que les entrées dans toute approche Deep Learning d'une tâche NLP auront probablement des vecteurs de mots en entrée.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons nos vecteurs de mot en entrée, regardons l'architecture de réseau que nous allons construire. L'aspect unique des données NLP est qu'il y a un aspect temporel. Chaque mot d'une phrase dépend grandement de ce qui précède et vient après. Afin de rendre compte de cette dépendance, nous utilisons un réseau de neurones récurrent.\n",
    "\n",
    "La structure du réseau neuronal récurrent est un peu différente de la NN feedforward traditionnelle que vous pourriez être accosté à voir. Le réseau feedforward se compose de noeuds d'entrée, d'unités cachées et de noeuds de sortie.\n",
    "\n",
    "![caption](Images/SentimentAnalysis17.png)\n",
    "\n",
    "La principale différence entre les réseaux de neurones feedforward et récurrents est l'aspect temporel de ces derniers. Dans les RNN, chaque mot d'une séquence d'entrée sera associé à un pas de temps spécifique. En effet, le nombre de pas de temps sera égal à la longueur de séquence maximale.\n",
    "\n",
    "![caption](Images/SentimentAnalysis18.png)\n",
    "\n",
    "Associé à chaque pas de temps est également un nouveau composant appelé un vecteur hidden state h <sub> t </sub>. D'un niveau élevé, ce vecteur cherche à encapsuler et résumer toutes les informations qui ont été vues dans les étapes de temps précédentes. Tout comme x <sub> t </sub> est un vecteur qui encapsule toutes les informations d'un mot spécifique, h <sub> t </sub> est un vecteur qui résume les informations des étapes précédentes.\n",
    "\n",
    "The hidden state est une fonction à la fois du vecteur de mot courant et du vecteur d'état caché au pas de temps précédent. Le sigma indique que la somme des deux termes passera par une fonction d'activation (normalement un sigmoïde ou un tanh).\n",
    "\n",
    "![caption](Images/SentimentAnalysis15.png)\n",
    "\n",
    "Les 2 termes W dans la formulation ci-dessus représentent des matrices de poids. Si vous regardez de près les superscripts, vous verrez qu'il y a une matrice de poids W <sup> X </sup> que nous allons multiplier avec notre entrée, et il y a une matrice de poids récurrente W <sup> H </sup> qui est multiplié avec le vecteur d'hidden state de pas de temps précédent. W <sup> H </sup> est une matrice qui reste la même pour tous les pas de temps, et la matrice de poids W <sup> X </sup> est différente pour chaque entrée. \n",
    "\n",
    "La magnitude de ces matrices de poids affecte la quantité que le vecteur d'état caché est affecté par le vecteur courant ou l'état caché précédent. Pour un exercice, jetez un oeil à la formule ci-dessus, et considérez comment h <sub> t </sub> changerait si W <sup> X </sup> ou W <sup> H </sup> avaient de grandes ou petites valeurs.\n",
    "\n",
    "Regardons un exemple rapide. Lorsque la magnitude de W <sup> H </sup> est grande et que la magnitude de W <sup> X </sup> est petite, on sait que h <sub> t </sub> est largement affecté par h <sub > t-1 </sub> et non affecté par x <sub> t </sub>. En d'autres termes, le vecteur d'état caché actuel voit que le mot courant est largement sans conséquence sur le résumé global de la phrase, et il aura donc essentiellement la même valeur que le vecteur au pas de temps précédent.\n",
    "\n",
    "Les matrices de poids sont mises à jour grâce à un processus d'optimisation appelé rétropropagation .\n",
    "\n",
    "Le hidden state vector au dernier moment, le pas est introduit dans un classificateur softmax binaire où il est multiplié par une autre matrice de pondération et soumis à une fonction softmax qui produit des valeurs entre 0 et 1, ce qui nous donne les probabilités de sentiment positif et négatif.\n",
    "\n",
    "![caption](Images/SentimentAnalysis16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory Units (LSTMs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short Term Memory Units est un module que vous pouvez placer dans des structures de réseaux de neurone reucrrent. À un niveau élevé, ils s'assurent que le vecteur de hidden state h est capable d'encapsuler des informations sur les dépendances à long terme dans le texte. Comme nous l'avons vu dans la section précédente, la formulation pour h dans les RNN traditionnels est relativement simple. Cette approche ne sera pas en mesure de relier efficacement des informations séparées par plus de deux pas de temps. Nous pouvons illustrer cette idée de gestion des dépendances à long terme par un exemple dans le domaine de la réponse aux questions. La fonction des modèles de réponse aux questions est de prendre un passage de texte et de répondre à une question sur son contenu. Regardons l'exemple suivant.\n",
    "\n",
    "![caption](Images/SentimentAnalysis4.png)\n",
    "\n",
    "Ici, nous voyons que la phrase du milieu n'a eu aucun impact sur la question qui a été posée. Cependant, il existe un lien fort entre les première et troisième phrases. Avec un RNN classique, le vecteur hidden state à la fin du réseau pourrait avoir stocké plus d'informations sur la phrase dog que sur la première phrase sur le nombre. Fondamentalement, l'ajout d'unités LSTM permet de déterminer les informations correctes et utiles qui doivent être stockées dans le vecteur d'état caché.\n",
    "\n",
    "En regardant les unités LSTM d'un point de vue plus technique, les unités prennent le vecteur de mot courant x <sub> t </sub> et émettent le vecteur d'état caché h <sub> t </sub>. Dans ces unités, la formulation de h <sub> t </sub> ser un peu plus complexe que celle d'un RNN typique. Le calcul est décomposé en 4 composants, une porte d'entrée, une porte d'oubli (forget), une porte de sortie et un nouveau conteneur de mémoire. \n",
    "\n",
    "![caption](Images/SentimentAnalysis10.png)\n",
    "\n",
    "Chaque porte prendra x <sub> t </sub> et h <sub> t-1 </sub> (non montré dans l'image) comme entrées et effectuera un calcul sur celles-ci pour obtenir des états intermédiaires. Chaque état intermédiaire est alimenté dans différents pipelines et finalement l'information est agrégée pour former h <sub> t </sub>. Par souci de simplicité, nous n'entrerons pas dans les formulations spécifiques de chaque portail, mais il convient de noter que chacune de ces portes peut être considérée comme différents modules au sein du LSTM ayant chacun des fonctions différentes. La porte d'entrée détermine combien d'accentuation mettre sur chacune des entrées, la porte d'oubli détermine les informations que nous allons jeter, et la porte de sortie détermine le h final <sub> t </sub> basé sur les états intermédiaires. Pour plus d'informations sur la compréhension des fonctions des différentes portes et les équations complètes, consultez Christopher Olah [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "En revenant sur le premier exemple avec la question \"Quelle est la somme des deux nombres?\", Le modèle devrait être formé sur des types similaires de questions et de réponses. Les unités LSTM seraient alors capables de réaliser que toute phrase sans nombre n'aura probablement pas d'impact sur la réponse à la question, et donc l'unité pourra utiliser sa porte d'oubli pour jeter les informations inutiles sur le chien, et plutôt Conservez les informations concernant les numéros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framing Sentiment Analysis as a Deep Learning Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme mentionné précédemment, la tâche de l'analyse des sentiments consiste à saisir une séquence d'entrée de mots et à déterminer si le sentiment est positif, négatif ou neutre. Nous pouvons séparer cette tâche spécifique (et la plupart des autres tâches PNL) en 5 composants différents.\n",
    "\n",
    "    1) Formation d'un modèle de génération de vecteurs de mots (tel que Word2Vec) ou chargement de vecteurs de mots pré-assemblés\n",
    "    2) Création d'une matrice d'ID pour notre ensemble d'entraînement\n",
    "    3) Création de graphe RNN (avec unités LSTM)\n",
    "    4) Training\n",
    "    5) Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premièrement, nous voulons créer nos vecteurs de mots. Pour simplifier, nous allons utiliser un modèle pré-entraîné.\n",
    "\n",
    "En tant que l'un des plus grandes entreprise de machine learning, Google a pu former un modèle Word2Vec sur un ensemble de données Google News qui contenait plus de 100 milliards de mots différents! De ce modèle, Google [a pu entrain 3 million vecteur de mots](https://code.google.com/archive/p/word2vec/#Pre-trained_word_and_phrase_vectors), chacun de dimentionnalité de 300.\n",
    "\n",
    "Dans un scénario idéal, nous utiliserions ces vecteurs, mais comme la matrice des vecteurs de mots est assez grande (3,6 Go!), Nous utiliserons une matrice avec moin de taille qui est entraînée en utilisant[GloVe](http://nlp.stanford.edu/projects/glove/), un modèle de génération de vecteur de mot similaire. La matrice contiendra 400 000 vecteurs de mots, chacun ayant une dimension de 50.\n",
    "\n",
    "Nous allons importer deux structures de données différentes, l'une sera une liste Python avec les 400 000 mots, et l'autre sera une matrice d'inclusion 400 000 x 50 qui contiendra toutes les valeurs de vecteur de mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juste pour s'assurer que tout a été correctement chargé, nous pouvons regarder les dimensions de la liste de vocabulaire et de la matrice d'intégration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons également chercher dans notre liste de mots un mot comme \"baseball\", puis accéder à son vecteur correspondant à travers la matrice d'intégration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.93270004,  1.04209995, -0.78514999,  0.91033   ,  0.22711   ,\n",
       "       -0.62158   , -1.64929998,  0.07686   , -0.58679998,  0.058831  ,\n",
       "        0.35628   ,  0.68915999, -0.50598001,  0.70472997,  1.26639998,\n",
       "       -0.40031001, -0.020687  ,  0.80862999, -0.90565997, -0.074054  ,\n",
       "       -0.87674999, -0.62910002, -0.12684999,  0.11524   , -0.55685002,\n",
       "       -1.68260002, -0.26291001,  0.22632   ,  0.713     , -1.08280003,\n",
       "        2.12310004,  0.49869001,  0.066711  , -0.48225999, -0.17896999,\n",
       "        0.47699001,  0.16384   ,  0.16537   , -0.11506   , -0.15962   ,\n",
       "       -0.94926   , -0.42833   , -0.59456998,  1.35660005, -0.27506   ,\n",
       "        0.19918001, -0.36008   ,  0.55667001, -0.70314997,  0.17157   ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseballIndex = wordsList.index('baseball')\n",
    "wordVectors[baseballIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons nos vecteurs, notre première étape consiste à prendre une phrase d'entrée puis à construire la représentation de son vecteur. Disons que nous avons la phrase d'entrée \"Je pensais que le film était incroyable et inspirant\". Pour obtenir les vecteurs de mots, nous pouvons utiliser la fonction de recherche d'intégration de Tensorflow. Cette fonction prend deux arguments, un pour la matrice d'intégration (la matrice wordVectors dans notre cas) et un pour les identifiants de chacun des mots. Le vecteur ids peut être considéré comme la représentation entière de l'ensemble d'apprentissage. C'est fondamentalement juste l'index de ligne de chacun des mots. Regardons un exemple rapide pour rendre cela concret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "maxSeqLength = 10 #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le pipeline de données peut être représenté ainsi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sortie 10 x 50 doit contenir les vecteurs de mot 50 dimensions pour chacun des 10 mots de la séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de créer la matrice d'identifiants pour l'ensemble de l'ensemble de formation, prenons d'abord un certain temps pour visualiser le type de données que nous avons. Cela nous aidera à déterminer la meilleure valeur pour définir notre longueur de séquence maximale. Dans l'exemple précédent, nous avons utilisé une longueur maximale de 10, mais cette valeur dépend largement des entrées que vous avez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset que nous allons utiliser est le jeu de données de review de Amazon food. Cet ensemble compte 35173 critiques de films, avec  critiques positives et 25 087 critiques positives. Chacune des critiques est stockée dans un fichier txt que nous devons analyser. Les commentaires négatives sont stockés dans un répertoire et les avis négatifs sont stockés dans un autre. Le morceau de code suivant déterminera le nombre total et moyen de mots dans chaque révision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 35173\n",
      "The total number of words in the files is 2713530\n",
      "The average number of words in the files is 77.14809655133199\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons également utiliser la bibliothèque Matplot pour visualiser ces données dans un format d'histogramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f88f00478d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de l'histogramme ainsi que du nombre moyen de mots par fichier, nous pouvons affirmer que la plupart des révisions tomberont sous 80 mots, on choisit alors la longueur de message de  150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons comment nous pouvons prendre un seul fichier et le transformer en notre matrice d'identifiants. C'est ce que l'un des commentaires ressemble au format de fichier texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We started using ghee in place of oils to help our daughter with severe autism.  She appears to be in better health and mood since this change.  We have tried four different varieties of ghee, but this is the best tasting and fits into more recipes because of the mild taste. There is a big price difference, but this  and is worth the extra money.\n"
     ]
    }
   ],
   "source": [
    "fname = positiveFiles[3] #Can use any valid index (not just 3)\n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, convertissons en une matrice d'identifiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    53,    551,    622,  68774,      6,    241,      3,  16237,\n",
       "            4,    275,    162,   1131,     17,   2546,  17179,     67,\n",
       "         1658,      4,     30,      6,    439,    360,      5,   5030,\n",
       "          108,     37,    511,     53,     33,    977,    133,    494,\n",
       "         8576,      3,  68774,     34,     37,     14, 201534,    254,\n",
       "        16701,      5,  10094,     75,     56,   8628,    113,      3,\n",
       "       201534,   6715,   4374,     63,     14,      7,    365,    626,\n",
       "         2333,     34,     37,      5,     14,   1089, 201534,   2004,\n",
       "          308,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "with open(fname) as f:\n",
    "    indexCounter = 0\n",
    "    line=f.readline()\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        try:\n",
    "            firstFile[indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "firstFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, faisons la même chose pour chacun de nos 35173 avis. Nous chargerons dans l'ensemble de formation de film et l'integerize pour obtenir une matrice 35173 x 150. C'était un processus coûteux en calcul, donc au lieu de vous faire exécuter toute la pièce, nous allons charger dans une matrice d'ID pré-calculée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "# fileCounter = 0\n",
    "# for pf in positiveFiles:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 \n",
    "#\n",
    "# for nf in negativeFiles:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 \n",
    " #Pass into embedding function and see if it evaluates. \n",
    "\n",
    "# np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.load('idsMatrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous trouverez ci-dessous quelques fonctions auxiliaires utiles lors de la formation ultérieure du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,25087)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(28087,35171)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(25087,28087)\n",
    "        if (num <= 27087):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous sommes prêts à commencer à créer notre graph Tensorflow. Nous devons d'abord définir certains hyperparamètres, tels que la taille du lot, le nombre d'unités LSTM, le nombre de classes de sortie et le nombre d'itérations d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme avec la plupart des graph Tensorflow, nous allons maintenant spécifier deux espaces réservés, un pour les entrées dans le réseau et un pour les étiquettes. La partie la plus importante de la définition de ces espaces réservés est de comprendre chacune de leurs dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'espace réservé des étiquettes représente un ensemble de valeurs, chacune [1, 0] ou [0, 1], selon que chaque exemple d'apprentissage est positif ou négatif. Chaque ligne de l'espace est représenté a une etiquette(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que nous avons notre espace réservé pour les données d'entrée, nous allons appeler la fonction tf.nn.lookup () afin d'obtenir nos vecteurs de mots. L'appel à cette fonction renvoie un tensor 3D de taille de lot de dimensionnalité par longueur de séquence maximale par dimensions de vecteur de mot. Afin de visualiser ce tenseur 3D, vous pouvez simplement considérer chaque point de données dans le tensor d'entrée entier comme le vecteur dimensionnel correspondant auquel il fait référence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons les données dans le format que nous voulons, regardons comment nous pouvons nourrir cette entrée dans un réseau LSTM. Nous allons appeler la fonction tf.nn.rnn_cell.BasicLSTMCell. Cette fonction prend un nombre entier pour le nombre d'unités LSTM que nous voulons. C'est l'un des hyperparamètres qui prendra un certain réglage pour déterminer la valeur optimale. Nous allons ensuite envelopper cette cellule LSTM dans un dropout layer pour éviter que le réseau ne fait pas un sur-apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous allons alimenter la cellule LSTM et le tensor 3D complet des données d'entrée dans une fonction appelée tf.nn.dynamic_rnn. Cette fonction est chargée de dérouler tout le réseau et de créer un chemin pour que les données circulent à travers le graphe RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En remarque, un autre choix d'architecture de réseau plus avancé consiste à empiler plusieurs cellules LSTM les unes sur les autres. C'est là que le vecteur d'état caché final du premier LSTM se nourrit dans le second. Empiler ces cellules est une excellente façon d'aider le modèle à conserver plus d'informations sur la dépendance à long terme, mais introduit également plus de paramètres dans le modèle, augmentant ainsi le temps d'entraînement, le besoin d'exemples d'entraînement supplémentaires et les risques d'overfitting. Pour plus d'informations sur la façon dont vous pouvez ajouter des LSTM empilés à votre modèle, consultez l'excellente [documentation](https://www.tensorflow.org/tutorials/recurrent#stacking_multiple_lstms) du Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première sortie de la fonction RNN dynamique peut être considérée comme le dernier vecteur de hidden state. Ce vecteur sera remodelé puis multiplié par une matrice de poids final et un terme de biais pour obtenir les valeurs de sortie finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous définirons des métriques de prédiction et de précision correctes pour suivre l'évolution du réseau. La formulation de prédiction correcte fonctionne en examinant l'index de la valeur maximale des deux valeurs de sortie, puis en vérifiant si elle correspond aux étiquettes d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons définir une standard de cross entropy loss avec une couche softmax placée au-dessus des valeurs de prédiction finale. Pour l'optimiseur, nous allons utiliser Adam et le taux d'apprentissage par défaut de 0,001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous souhaitez utiliser Tensorboard pour visualiser les valeurs de perte et de précision, vous pouvez également exécuter et modifier le code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choisir les bonnes valeurs pour vos hyperparamètres est une partie cruciale de la formation efficace des réseaux de neurone profonds. Vous constaterez que vos courbes de perte d'entraînement peuvent varier selon votre choix d'optimiseur (Adam, Adadelta, SGD, etc.), le taux d'apprentissage et l'architecture réseau. Avec les RNN et les LSTM en particulier, d'autres facteurs importants incluent le nombre d'unités LSTM et la taille des vecteurs de mots.\n",
    "\n",
    "* Learning Rate: RNN sont tristement célèbres pour être diffultes à former en raison du grand nombre de pas de temps qu'ils ont. Le taux d'apprentissage devient extrêmement important car nous ne voulons pas que nos valeurs de poids fluctuent énormément en raison d'un taux d'apprentissage élevé, et nous ne voulons pas d'un processus d'entraînement lent en raison d'un faible taux d'apprentissage. La valeur par défaut de 0,001 est un bon point de départ. Vous devriez augmenter cette valeur si la perte d'entraînement change très lentement, et diminuer si la perte est instable.\n",
    "\n",
    "* Optimizer: Il n'y a pas de choix consensuel parmi les chercheurs, mais Adam a été très populaire en raison de sa propriété de taux d'apprentissage adaptatif (retenez bien optimal learning rates diffère avec le choix de l'optimizer).\n",
    "\n",
    "* Number of LSTM units: Cette valeur dépend en grande partie de la longueur moyenne de vos textes d'entrée. Alors qu'un plus grand nombre d'unités fournit plus d'expressivité pour le modèle et permet au modèle de stocker plus d'informations pour des textes plus longs, le réseau prendra plus de temps à s'entraîner et sera coûteux en termes de calcul.\n",
    "\n",
    "* Word Vector Size: Les dimensions pour les vecteurs de mots vont généralement de 50 à 300. Une taille plus grande signifie que le vecteur est capable d'encapsuler plus d'informations sur le mot, mais vous devriez également vous attendre à un modèle plus coûteux en calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée de base de la boucle d'apprentissage est que nous définissons d'abord une session Tensorflow. Ensuite, nous chargeons un lot de critiques et leurs étiquettes associées. Ensuite, nous appelons la fonction `run` de la session. Cette fonction a deux arguments. Le premier s'appelle l'argument \"fetchs\". Il définit la valeur que nous sommes intéressés par l'informatique. Nous voulons que notre optimiseur soit calculé car c'est le composant qui minimise notre fonction de perte. Le deuxième argument est où nous entrons notre `feed_dict`. Cette structure de données est l'endroit où nous fournissons des intrants à tous nos espaces réservés. Nous devons nourrir notre batsh de review et notre batsh d'étiquettes (label). Cette boucle est ensuite répétée pour un nombre déterminé d'itérations d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au lieu de former le réseau dans ce cahier (ce qui prendra au moins quelques heures), nous chargerons dans un modèle pré-entraîné. [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/pretrained_lstm_1.ckpt-10000\n",
      "saved to models/pretrained_lstm_1.ckpt-20000\n",
      "saved to models/pretrained_lstm_1.ckpt-30000\n",
      "saved to models/pretrained_lstm_1.ckpt-40000\n",
      "saved to models/pretrained_lstm_1.ckpt-50000\n",
      "saved to models/pretrained_lstm_1.ckpt-60000\n",
      "saved to models/pretrained_lstm_1.ckpt-70000\n",
      "saved to models/pretrained_lstm_1.ckpt-80000\n",
      "saved to models/pretrained_lstm_1.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    " sess = tf.InteractiveSession()\n",
    " saver = tf.train.Saver()\n",
    " sess.run(tf.global_variables_initializer())\n",
    "\n",
    " for i in range(iterations):\n",
    "    #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "#    #Save the network every 10,000 training iterations\n",
    "    if (i % 10000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm_1.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    " writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La precision(accuracy) et Loss sont représentés dans les figures suivantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis6.png)\n",
    "![caption](Images/SentimentAnalysis7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En regardant les courbes d'entraînement ci-dessus, il semble que la formation du modèle se passe bien. La perte diminue régulièrement et la précision approche de 100%. Toutefois, lors de l'analyse des courbes d'entraînement, nous devrions également accorder une attention particulière à la possibilité que notre modèle overfit l'ensemble de données d'apprentissage. Le surapprentissage est un phénomène courant dans l'apprentissage automatique où un modèle devient si adapté aux données d'apprentissage qu'il perd la possibilité de généraliser à l'ensemble de test. Cela signifie que la formation d'un réseau jusqu'à ce que vous atteignez 0 perte d'entraînement ne soit pas la meilleure façon d'obtenir un modèle précis qui fonctionne bien sur des données qu'il n'a jamais vues auparavant. L'arrêt précoce est une technique intuitive couramment utilisée avec les réseaux LSTM pour lutter contre ce problème. L'idée de base est que nous formons le modèle sur notre ensemble d'entraînement, tout en mesurant de temps en temps ses performances sur l'ensemble de test. Une fois que l'erreur de test cesse de diminuer régulièrement et commence à augmenter à la place, vous saurez arrêter l'entraînement, car c'est un signe que le réseau a commencé à être overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le chargement d'un modèle pré-entraîné implique la définition d'une autre session Tensorflow, la création d'un objet Saver, puis l'utilisation de cet objet pour appeler la fonction de restauration. Cette fonction prend en compte 2 arguments, un pour la session en cours, et un pour le nom du modèle sauvegardé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous allons charger quelques critiques de films de notre ensemble de test. Rappelez-vous, ce sont des critiques que le modèle n'a pas été formé et n'a jamais vu auparavant. La précision pour chaque batch de test peut être vu lorsque vous exécutez le code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 91.6666686535\n",
      "Accuracy for this batch: 91.6666686535\n",
      "Accuracy for this batch: 66.6666686535\n",
      "Accuracy for this batch: 79.1666686535\n",
      "Accuracy for this batch: 79.1666686535\n",
      "Accuracy for this batch: 75.0\n",
      "Accuracy for this batch: 75.0\n",
      "Accuracy for this batch: 75.0\n",
      "Accuracy for this batch: 87.5\n",
      "Accuracy for this batch: 83.3333313465\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
